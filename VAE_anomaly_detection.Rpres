<style>
.footer {
    position: fixed; 
    top: 90%;
    text-align:right; 
    width:100%;
}

.banner {
    position: fixed; 
    top: 0%;
    text-align:right; 
    width:100%;
}

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 1.6em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.small-code pre code {
  font-size: .85em;
}

.reveal .smalltext {
  font-size: 0.75em;
}

</style>


Variational autoencoders for anomaly detection
========================================================
author: Sigrid Keydana, Trivadis
date: 2017/16/09
autosize: true
incremental:false
width: 1400
height: 900


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Variational autoencoders: Setting the scene</h1>


Welcome to the world of deep neural networks
========================================================

&nbsp;

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE,
                      cache = TRUE)
```

```{r}
library(ggplot2)
library(dplyr)
library(keras)
```

<figure>
    <img src='networkZooPoster_part.png' width='70%'/>
    <figcaption>Source: [1]</figcaption>
</figure>


<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


No magic involved: autoencoders
========================================================

&nbsp;

- neither a supervised nor an unsupervised, but a _self-supervised_ technique
- not conducive to learning interesting features / abstractions
- useful for applications such as image denoising and dimensionality reduction for visualization

<figure>
    <img src='autoencoder_schema.jpg' width='40%' />
    <figcaption>Source: [2]</figcaption>
</figure>


&nbsp;

<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Anomalies
========================================================
incremental: true

&nbsp;

An outlier is...
_an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism_ [3]

&mdash;> we need a probabilistic approach

Enter: generative stochastic models
  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Latent variable models
========================================================


&nbsp;

Maximize:

$P(X) = \int P(X|z;\theta) P(z) dz$
  
&nbsp;

<figure>
    <img src='vae_gm.png' width='40%' />
    <figcaption>Source: [4]</figcaption>
</figure>


<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Questions
========================================================

&nbsp;

- How do we get a mapping from easy-to-sample-from $z$'s to the empirical output ($X$)?

- How do we make sure our $z$'s are in the appropriate range to yield the $X$'s?


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Variational autoencoder
========================================================

&nbsp;

- Sample from $P(Z) = \mathcal N(z|0,I)$

- Let the network learn the decoder: $P(X|z;\theta) = \mathcal N(X|f(z,\theta), \sigma^2 I)$

- Let the network learn the encoder: $Q(z|X)$


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Variational autoencoder objective
========================================================


&nbsp;

Maximize the _variational lower bound_

$$E_{z \sim Q}[log P(X|z)] - \mathcal D[Q(z|X)||P(z)] $$

where the terms are

1) the reconstruction error

2) the Kullback-Leibler divergence between the approximate posterior and the prior for $z$
  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


VAE in a nutshell
========================================================


&nbsp;

<figure>
    <img src='vae_doersch.png' width='80%' />
    <figcaption>Source: [4]</figcaption>
</figure>

  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

How can we use this?
========================================================


&nbsp;

Generate stuff (e.g., images)
  
<figure>
    <img src='vae_generate.png' width='30%' />
    <figcaption>Source: [4]</figcaption>
</figure>

<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

How can we use this for ANOMALY DETECTION?
========================================================

&nbsp;

- VAE models the distribution, not the values

- anomalies are seen as coming from a _different process / distribution_, so...

- can diagnose as anomalies those cases that have _high reconstruction error / low reconstruction probability_


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Let's see this in practice!
========================================================


&nbsp;

- MNIST, what else ;-)

- fraud detection

- network intrusion detection
  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Before we dive in: What's the technology employed?
========================================================

&nbsp;

- <a href="http://keras.io">Keras</a>, used from R, via the <a href="https://keras.rstudio.com/">bindings provided by Rstudio</a>:
    - used for all models

- <a href="https://deeplearning4j.org">DL4J</a>: 
    - for reconstruction-probability based outliers in MNIST

  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Variational-autoencoding MNIST</h1>


Ubiquitous MNIST
========================================================

&nbsp;

MNIST handwritten digits database

<figure>
    <img src='mnist.jpg' width='40%' />
</figure>


<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


MNIST latent space
========================================================

&nbsp;


<table>
<tr>
<td><img src='mnist_latent.png' /></td>
<td><img src='mnist_generated.png' /></td>
</tr>
</table>

  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Best and worst reconstructions
========================================================

&nbsp;

using reconstruction probability

<figure>
    <img src='mnist_best_worst.png' width='50%' />
</figure>


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

So which are the anomalies?
========================================================

&nbsp;

- have to define cut-off points
- as with other machine learning /statistics-based algorithms
- so is this a problem?
- the more urgent question really is...
  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

How well does this work with real-world datasets?
========================================================

&nbsp;

- MNIST is 28*28 pixels, grayscale ==> super homogeneous data!
- best supervised learning error rate as of today (test set): 0.23% ==> super simple!
    
How about real datasets with different types of variables of different scale?

Let's see how far we'll get.
  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Real-world example: fraud detection</h1>

TBD
========================================================

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


TBD
========================================================

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

TBD
========================================================

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

TBD
========================================================

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

TBD
========================================================

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

TBD
========================================================

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

TBD
========================================================

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Fraudulent Transactions (Torgo, Data Mining with R, 2017)
========================================================

&nbsp;

- data "from an undisclosed source", anonymized
- transactions reported by salespeople
    - salesman id
    - product id
    - quantity sold
    - total value of sale
    - fraud yes/no/unknown
- total transactions 401,146 - of these, 385414 unknown!
    
  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>
Sources
========================================================
class:smalltext

&nbsp;

[1] Asimov Institute, <a href='http://www.asimovinstitute.org/neural-network-zoo/'>The Neural Network Zoo.</a>

[2] Keras blog, <a href='https://blog.keras.io/building-autoencoders-in-keras.html'>Building autoencoders in Keras.</a>

[3] Hawkins, D. (1980), Identification of outliers. Chapman and Hall, London.

[4] Doersch, C.,<a href="https://arxiv.org/pdf/1606.05908.pdf">Tutorial on Variational Autoencoders.</a>  

[5] An, J. & Cho, S.,<a href="http://dm.snu.ac.kr/static/docs/TR/SNUDM-TR-2015-03.pdf">Variational Autoencoder based Anomaly Detection using Reconstruction Probability</a>

[6] Torgo, L. (2017), Data Mining with R, 2nd ed..