<style>
.footer {
    position: fixed; 
    top: 90%;
    text-align:right; 
    width:100%;
}

.banner {
    position: fixed; 
    top: 0%;
    text-align:right; 
    width:100%;
}

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 1.6em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.small-code pre code {
  font-size: .85em;
}

.reveal .smalltext {
  font-size: 0.75em;
}

</style>


Variational autoencoders for anomaly detection
========================================================
author: Sigrid Keydana, Trivadis
date: 2017/16/09
autosize: true
incremental:false
width: 1400
height: 900


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Variational autoencoders: Setting the scene</h1>


Welcome to the world of deep neural networks
========================================================

&nbsp;

<figure>
    <img src='networkZooPoster_part.png' width='70%'/>
    <figcaption>Source: [1]</figcaption>
</figure>


<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


No magic involved: autoencoders
========================================================

&nbsp;

- neither a supervised nor an unsupervised, but a _self-supervised_ technique
- not conducive to learning interesting features / abstractions
- useful for applications such as image denoising and dimensionality reduction for visualization

<figure>
    <img src='autoencoder_schema.jpg' width='40%' />
    <figcaption>Source: [2]</figcaption>
</figure>


&nbsp;

<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Anomalies
========================================================
incremental: true

&nbsp;

An outlier is...
_an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism_ [3]

&mdash;> we need a probabilistic approach

Enter: generative stochastic models
  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Latent variable models
========================================================


&nbsp;

Maximize:

$P(X) = \int P(X|z;\theta) P(z) dz$
  
&nbsp;

<figure>
    <img src='vae_gm.png' width='40%' />
    <figcaption>Source: [4]</figcaption>
</figure>


<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Questions
========================================================

&nbsp;

- How do we get a mapping from easy-to-sample-from $z$'s to the empirical output ($X$)?

- How do we make sure our $z$'s are in the appropriate range to yield the $X$'s?


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Variational autoencoder
========================================================

&nbsp;

- Sample from $P(Z) = \mathcal N(z|0,I)$

- Let the network learn the decoder: $P(X|z;\theta) = \mathcal N(X|f(z,\theta), \sigma^2 I)$

- Let the network learn the encoder: $Q(z|X)$


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Variational autoencoder objective
========================================================


&nbsp;

Maximize the _variational lower bound_

$$E_{z \sim Q}[log P(X|z)] - \mathcal D[Q(z|X)||P(z)] $$

where the terms are

1) the reconstruction error

2) the Kullback-Leibler divergence between the approximate posterior and the prior for $z$
  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


VAE in a nutshell
========================================================


&nbsp;

<figure>
    <img src='vae_doersch.png' width='80%' />
    <figcaption>Source: [4]</figcaption>
</figure>

  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

How can we use this?
========================================================


&nbsp;

Generate stuff (e.g., images)
  
<figure>
    <img src='vae_generate.png' width='30%' />
    <figcaption>Source: [4]</figcaption>
</figure>

<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

How can we use this for ANOMALY DETECTION?
========================================================

&nbsp;

- VAE models the distribution, not the values

- anomalies are seen as coming from a _different process / distribution_, so...

- can diagnose as anomalies those cases that have _high reconstruction error / low reconstruction probability_


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


Let's see this in practice!
========================================================


&nbsp;

- MNIST, what else ;-)

- fraud detection

- network intrusion detection
  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Variational-autoencoding MNIST</h1>


Anomalies
========================================================
incremental: true

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Anomalies
========================================================
incremental: true

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Anomalies
========================================================
incremental: true

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Anomalies
========================================================
incremental: true

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Anomalies
========================================================
incremental: true

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

Anomalies
========================================================
incremental: true

&nbsp;


  
<div class="banner">
<img src='tri_logo_high.jpg' border=0 width='200px'>
</div>

<div class="footer">
<img src='cube3.png' border=0 width='122px'>
</div>

========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>One step back: What's deep learning?</h1>



Sources
========================================================

&nbsp;

[1] Asimov Institute, <a href='http://www.asimovinstitute.org/neural-network-zoo/'>The Neural Network Zoo.</a>

[2] Keras blog, <a href='https://blog.keras.io/building-autoencoders-in-keras.html'>Building autoencoders in Keras.</a>

[3] Hawkins, D. (1980), Identification of outliers. Chapman and Hall, London.

[4] Carl Doersch, <a href="https://arxiv.org/pdf/1606.05908.pdf">Tutorial on Variational Autoencoders.</a>  
